{"name":"Baxter Builds a Tower","tagline":"Using a Baxter Robot with AR Tags to stack blocks into a Tower","body":"# Introduction\r\nIn **Baxter Builds a Tower**, the end goal is to allow a [Baxter robot] (http://www.rethinkrobotics.com/) to autonomously pick up toy blocks in any desired order and then stack them vertically to create a tower.\r\n\r\n![Toy blocks being stacked](http://ucbbaxtertower.github.io/BaxterTower/media/tower1.jpg)\r\n\r\n**Baxter Builds a Tower** is an interesting project because it combines both robust computer vision and good kinematics/path planning for success. In our traditional mechanical engineering training, we often deal with the latter instead of the former. Therefore, **Baxter Builds a Tower** is a very attractive project for us because we get the rare chance to work in both realms of robotics at once. In the course of this project, we had to overcome the limitations of off the shelf computer vision packages as well as the hardware limitations in Baxter’s end effector accuracy. In later sections, we will detail how we overcame these obstacles.\r\n\r\nThe work developed in this project can be useful in car junkyards when cars arrive and need to be stacked/stored. Upon further extension, the work here can also aid in assembly line quality assurance tasks, such as removing defective products from an assembly line and storing them in a separate location for further processing. More importantly, Baxter playing with toy blocks is a great boon towards promoting STEM to the younger audience.\r\n\r\n\r\n# Design\r\n### Criterias\r\n* Identify toy blocks through computer vision\r\n* Retrieve toy blocks with robotic arm\r\n* Place toy blocks down and successively stack blocks vertically into a tower\r\n* Repeat\r\n* **Baxter will autonomously pick up many toy blocks and successively stack them to make a tower**\r\n\r\nTo achieve the above, we decided to use [AR Tags](http://wiki.ros.org/ar_track_alvar) and [ROS MoveIt](http://moveit.ros.org/) pathplanner in order to manipulate toy blocks conveniently sold in various toy stores. We decided to use AR Tags to simplify the task of computer vision, but at the expense of only manipulating objects with large planar faces that can support an AR Tag. Moreover, we had to use cameras located on Baxter’s hand in order to perceive the toy blocks because the head-mounted camera is too far away to accurately detect AR Tags. Consequently, the robot workspace decreased accordingly because of the camera’s decreased field of view. Finally, we intelligently selected the location of toy blocks and arm motion trajectory to minimize the chance of toy block to robot collisions. By doing so, we can avoid importing CAD models of the environment to RVIZ  and save on computation time in path planning.\r\n\r\nThe simplifying design choices made here probably would not suffice in the real world. The decision to use AR Tags definitely constrains the realistic sizes of objects we can manipulate with. Moreover, using hand-mounted cameras, instead of head-mounted or externally mounted, cameras definitely limits the workspace and forces us to be tied to platforms, like Baxter, with hand-mounted cameras. Finally, not allowing Baxter to be cognizant of its environment means that environments would have to be highly controlled and largely static, characteristics that are too idealized compared to the real world.\r\n\r\n# Implementation\r\n\r\nTo complete our project, we used the standard Baxter robotic platform with the suction actuator attachment. We manipulated with toy blocks that were 1.5” cubes and made of wood. On one face of each toy block, we adhered AR Tags to enable Baxter to uniquely identify each block.\r\n\r\nThe [ROS](http://www.ros.org/) framework is the software glue that ties all of the hardware together.\r\n\r\n1. Initialization of AR tracking and MoveIt server as well as robot and camera enabling\r\n2. Move Baxter’s right arm over toy blocks to localize toy blocks location in the base frame\r\n3. The publisherMarkerInfo node will publish to found_markers with an array of all markers that were identified\r\n4. The master node decides which AR Tag to retrieve and computes the transform between the base frame and the AR Tag frame\r\n5. The translational component of the transform is used by MoveIt as a destination waypoint.\r\n6 .The master node also appends a series of intermediate waypoints to help anchor MoveIt’s trajectory so that the arm moves away from the toy block in the z-direction and approaches the stack of toy blocks from the z-direction \r\n7. Baxter’s right arm is moved over toy blocks again to get an updated reading of the remaining toy blocks’ location\r\n8. Steps 4 to 6 are repeated until all blocks have been retrieved and stacked\r\n\r\n\r\n# Results\r\n\r\nThe video below summarizes our work. Concisely, we successfully:\r\n* Implemented autonomously tracking and selection of toy blocks, marked by AR Tags\r\n* Implemented autonomous path planning between toy block retrieval and placement\r\n* Demonstrated an ability to stack toy blocks successively 10 times in a vertical fashion\r\n\r\n# Conclusion\r\nCompared to our original goals, we have completed most of our objectives except the ability to build more complicated structures, such as a house. This can be attributed to Baxter’s intrinsic positional inaccuracy in its arm which hinders its ability to place toy blocks in a structurally stable fashion.\r\n\r\nMoreover, Baxter’s hand-mounted cameras exhibited a fish-eye effect which led to errors in perceiving the location of the AR Tags. This distortion could have been easily fixed by a proper calibration, but Baxter does not readily support such services.\r\n\r\nIf we had additional time, we would invest in an external camera solution to expand our viewable workspace as well as increase reliability in our sensing through properly calibrated cameras. Plus, we calibrated our robot towards a particular table height (i.e. the Cory 119 table) in order to keep our problem tractable.\r\n\r\nFor a summary, please see our [slides](www.google.com).\r\n\r\n\r\n# Team\r\n### Hsien-Chung Lin\r\n### Te Tang\r\n### Dennis Wai\r\nDennis Wai is a Mechanical Engineering student that is a little too interested in Electrical Engineering and Computer Science. He likes robots and often can be found volunteering with [Pioneers in Engineering](pioneers.berkeley.edu), a student group focused on delivering robotics experience to underserved Bay Area communities. For this other interests, you can check out his [website](denniswai.com)\r\n\r\n# Additional Materials\r\n* Source Code on [Github](https://github.com/ucbBaxterTower/BaxterTower)\r\n* [Toy blocks](http://www.toysrus.com/product/index.jsp?productId=37649076&cp=&parentPage=search)\r\n* [ROS](www.ros.org)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}